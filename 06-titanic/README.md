# ğŸš¢ Titanic - Machine Learning from Disaster

## ğŸ“Œ Overview

The **Titanic competition** is one of the most famous beginner challenges on Kaggle.
It uses real passenger data from the **RMS Titanic (1912)** tragedy to predict survival outcomes.

Competition link: [Titanic on Kaggle](https://www.kaggle.com/competitions/titanic)

This project applies a full **ML pipeline** (EDA â†’ Preprocessing â†’ Feature Engineering â†’ Modeling â†’ Evaluation â†’ Submission).

## ğŸ“‚ Dataset Information

**Training set:** 891 passengers (with labels).
**Test set:** 418 passengers (without labels).
**Target variable:**

* `Survived` â†’ 1 = Survived, 0 = Did not survive.

### ğŸ”‘ Key Features

* **Pclass** â†’ Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).
* **Sex** â†’ Gender (male/female).
* **Age** â†’ Passenger age in years.
* **SibSp** â†’ Number of siblings/spouses aboard.
* **Parch** â†’ Number of parents/children aboard.
* **Fare** â†’ Ticket fare.
* **Embarked** â†’ Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).
* **Cabin** â†’ Cabin number (many missing values).
* **Name, Ticket, PassengerId** â†’ Passenger identifiers.

## ğŸ¯ Objectives

* Perform **EDA**: distributions, missing values, survival rates by feature.
* **Feature Engineering**: handle missing values, create `FamilySize`, extract titles from names, log-transform `Fare`.
* Train classification models: Logistic Regression, Random Forest, Gradient Boosting, XGBoost, LightGBM.
* Evaluate model performance with Accuracy, ROC-AUC, Confusion Matrix.
* Generate predictions for Kaggle submission.

## ğŸ›  Methodology & Tools

* **Data Cleaning:** missing value imputation (`Age`, `Embarked`, `Fare`), drop irrelevant features (`Ticket`, `Cabin`).
* **EDA & Visualization:** Matplotlib, Seaborn.
* **Feature Engineering:** categorical encoding, derived features (titles, family size).
* **Modeling:** Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, XGBoost.
* **Evaluation:** Accuracy (for Kaggle), ROC-AUC, F1-score.

## ğŸ“Š Key Insights

* **Sex** is the strongest predictor: survival rate of females is significantly higher.
* **Pclass** is critical: 1st class passengers had much higher survival probability.
* ğŸ‘¨â€ğŸ‘©Passengers traveling with family (`FamilySize > 1`) had higher chances of survival.
* Higher **Fare** correlates with higher survival probability.
* **Embarked** also plays a role: passengers from Cherbourg had better survival rates.

## ğŸš€ Next Steps

* Improve feature engineering (cabin grouping, ticket family extraction).
* Perform **hyperparameter tuning** with Optuna.
* Use **stacking / ensemble models** for higher Kaggle scores.
* Deploy as a simple web app for Titanic survival prediction.

## ğŸ‘¤ Author

* **Name:** ÄÃ o Minh Thuáº¥n
* **GitHub:** [daominhthuan42](https://github.com/daominhthuan42)
